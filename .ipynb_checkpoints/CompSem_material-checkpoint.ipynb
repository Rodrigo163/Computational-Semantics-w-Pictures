{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# CompSem material\n",
    "Notebook to keep at hand all CompSem related functions that we might need for our final project.\n",
    "To try this functions we have to use the Uni's JupyterHub server, so they won't work here. This is just a reference place to keep all useful code, preferably with the example outputs taken from executing it in the server. \n",
    "\n",
    "Proposed workflow:\n",
    "* Make sure that you are using the latest version of this file\n",
    "* Upload this file to the server\n",
    "* Edit, add and run whatever new material you found\n",
    "* Download the file and save it here, deleting the previous version. In case maaany changes were done then leave the previous version but add the current date to its name, to know that it's a deprecated version of the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from __future__ import division\n",
    "import codecs\n",
    "import json\n",
    "from itertools import chain, permutations, combinations\n",
    "from collections import Counter, defaultdict\n",
    "import configparser\n",
    "import os\n",
    "import random\n",
    "from textwrap import fill\n",
    "import scipy\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "import spacy\n",
    "from annoy import AnnoyIndex\n",
    "from nltk.parse import CoreNLPParser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "from IPython.display import Latex, display\n",
    "\n",
    "pd.set_option('max_colwidth', 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Load up config file (needs path; adapt env var if necessary); local imports\n",
    "\n",
    "# load config file, set up paths, make project-specific imports\n",
    "config_path = os.environ.get('VISCONF')\n",
    "if not config_path:\n",
    "    # try default location, if not in environment\n",
    "    default_path_to_config = '../../clp-vision/Config/default.cfg'\n",
    "    if os.path.isfile(default_path_to_config):\n",
    "        config_path = default_path_to_config\n",
    "\n",
    "assert config_path is not None, 'You need to specify the path to the config file via environment variable VISCONF.'        \n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "with codecs.open(config_path, 'r', encoding='utf-8') as f:\n",
    "    config.read_file(f)\n",
    "\n",
    "corpora_base = config.get('DEFAULT', 'corpora_base')\n",
    "preproc_path = config.get('DSGV-PATHS', 'preproc_path')\n",
    "dsgv_home = config.get('DSGV-PATHS', 'dsgv_home')\n",
    "\n",
    "\n",
    "sys.path.append(dsgv_home + '/Utils')\n",
    "from utils import icorpus_code, plot_labelled_bb, get_image_filename, query_by_id\n",
    "from utils import plot_img_cropped, plot_img_ax, invert_dict, get_a_by_b\n",
    "sys.path.append(dsgv_home + '/WACs/WAC_Utils')\n",
    "from wac_utils import create_word2den, is_relational\n",
    "sys.path.append(dsgv_home + '/Preproc')\n",
    "from sim_preproc import load_imsim, n_most_sim\n",
    "\n",
    "sys.path.append('../Common')\n",
    "from data_utils import load_dfs, plot_rel_by_relid, get_obj_bb, compute_distance_objs\n",
    "from data_utils import get_obj_key, compute_relpos_relargs_row, get_all_predicate\n",
    "from data_utils import compute_distance_relargs_row, get_rel_type, get_rel_instances\n",
    "from data_utils import compute_obj_sizes_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Selecting which DataFrames to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Load up preprocessed DataFrames. Slow!\n",
    "# These DataFrames are the result of pre-processing the original corpus data,\n",
    "# as per dsg-vision/Preprocessing/preproc.py\n",
    "\n",
    "df_names = [\n",
    "#             'saiapr_bbdf',\n",
    "#            'mscoco_bbdf',\n",
    "#            'mscoco_catsdf',\n",
    "#            'cococapdf',\n",
    "#             'vgregdf',\n",
    "#             'vgimgdf',\n",
    "#             'vgreldf',\n",
    "#             'vgobjdf',\n",
    "             'vgpardf' \n",
    "#             'flickr_bbdf',\n",
    "#             'flickr_capdf',\n",
    "#             'flickr_objdf',\n",
    "#             'ade_imgdf',\n",
    "#             'ade_objdf',\n",
    "#             'cub_bbdf',\n",
    "#             'cub_attrdf',\n",
    "#             'cub_partdf',\n",
    "#             'cub_capdf'\n",
    "           ]\n",
    "df = load_dfs(preproc_path, df_names)\n",
    "\n",
    "# a derived DF, containing only those region descriptions which I was able to resolve\n",
    "# df['vgpregdf'] = df['vgregdf'][df['vgregdf']['pphrase'].notnull() & \n",
    "#                                (df['vgregdf']['pphrase'] != '')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Corpus analysis\n",
    "Getting an idea of NL/V characteristics of the corpus. Sizes, distributions, general statistics, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "current_df = df['vgpardf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "current_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "current_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Language Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "full_sentences = np.asarray(df.paragraph)\n",
    "print(len(full_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#tokenizing our sentences\n",
    "tokenized_sentences = [nltk.tokenize.word_tokenize(sen.lower()) for sen in full_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#Avg sentence length\n",
    "sent_lens = [len(sentence) for sentence in tokenized_sentences]\n",
    "mean = np.mean(sent_lens)\n",
    "median = np.median(sent_lens)\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = sns.distplot(sent_lens, norm_hist=False, kde=False)\n",
    "ax.set(xlabel='sentence length', ylabel='')\n",
    "\n",
    "plt.axvline(mean,color='r', linestyle='--')\n",
    "plt.axvline(median,color='g', linestyle='--')\n",
    "\n",
    "plt.legend({f'Mean = {mean.round()}':mean,f'Median = {median.round()}':median})\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#Word and POS tag freq analysis\n",
    "words = list(chain.from_iterable(tokenized_sentences))\n",
    "fdist = nltk.FreqDist(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#Hapaxes\n",
    "hapaxes = fdist.hapaxes()\n",
    "len(hapaxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame([random.sample(hapaxes, 10), random.sample(hapaxes, 10), random.sample(hapaxes, 10), random.sample(hapaxes, 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#getting POS tags\n",
    "pos_sents = [nltk.pos_tag(i) for i in tokenized_sentences]\n",
    "pos_sents = [item for sublist in pos_sents for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "nouns = [word[0] for word in pos_sents if word[1][0] == 'N']\n",
    "adjectives = [word[0] for word in pos_sents if word[1][0] == 'J']\n",
    "verbs = [word[0] for word in pos_sents if word[1][0] == 'V']\n",
    "\n",
    "nouns_mc = nltk.FreqDist(nouns).most_common(15)\n",
    "verbs_mc = nltk.FreqDist(verbs).most_common(15)\n",
    "adjectives_mc = nltk.FreqDist(adjectives).most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#pretty plots\n",
    "\n",
    "fig, axes = plt.subplots(3)\n",
    "\n",
    "fig.set_size_inches(15, 10)\n",
    "\n",
    "sns.barplot(x=[i[0] for i in nouns_mc], y=[i[1] for i in nouns_mc], \n",
    "            ax=axes[0])\n",
    "\n",
    "\n",
    "sns.barplot(x=[i[0] for i in adjectives_mc], y=[i[1] for i in adjectives_mc], \n",
    "            ax=axes[1])\n",
    "\n",
    "\n",
    "sns.barplot(x=[i[0] for i in verbs_mc], y=[i[1] for i in verbs_mc], \n",
    "            ax=axes[2], label='oksffas')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Image Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
